# %% [markdown]
# # Diffusion Model GPU Optimizations Demo
# 
# This notebook demonstrates several GPU optimizations applied to a diffusion model pipeline—similar to what underpins Stable Diffusion. 
# 
# **Overview:**  
# Stable Diffusion is a text-to-image generative model that iteratively denoises a latent representation. It involves several stages:
# 
# 1. **Text Embedding:** Converts text prompts into high-dimensional embeddings (e.g., via CLIP) that condition the model.
# 2. **Noise Generation:** Generates a random noise tensor that serves as the starting point in latent space.
# 3. **Denoising (UNet) Operations:** Iteratively refines the noisy latent representation via a UNet architecture.
# 4. **Image Decoding:** Transforms the final denoised latent representation into an RGB image.
# 5. **Extra GPU Optimizations:** Additional optimizations such as Winograd convolution and partially fused softmax that help accelerate the overall process.
#
# In each section, we provide code examples, performance metrics, and visualizations to illustrate the potential speedups available with GPU optimizations.
#

# %% [markdown]
# ## 1. Text Embedding Demo with PCA Visualization
#
# **Purpose:**  
# In Stable Diffusion, a text embedder (often based on CLIP) converts text prompts into high-dimensional vectors. These embeddings condition the denoising process.
#
# **Demo Description:**  
# We simulate multiple text embeddings for different prompts, then reduce their dimensionality using PCA for a 2D visualization. This demonstrates how similar prompts might cluster together in the latent space.
#

# %% [code]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Simulate a text embedder: returns a 512-dimensional embedding for a given prompt.
def simulate_text_embedding(prompt: str, embedding_dim=512):
    np.random.seed(len(prompt))  # For consistency based on prompt length
    embedding = np.random.randn(embedding_dim)
    return embedding

# List of sample prompts
prompts = [
    "A futuristic cityscape at sunset.",
    "A peaceful forest in autumn.",
    "A busy urban street at night.",
    "An abstract painting of emotions.",
    "A serene beach with crystal clear water."
]

# Generate embeddings for each prompt.
embeddings = np.array([simulate_text_embedding(p) for p in prompts])

# Reduce dimensions using PCA to visualize in 2D.
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings)

plt.figure(figsize=(6,4))
plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], color='purple')
for i, prompt in enumerate(prompts):
    plt.annotate(f"P{i+1}", (embeddings_2d[i,0], embeddings_2d[i,1]))
plt.title("2D PCA Projection of Text Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.grid(True)
plt.show()

# %% [markdown]
# **Explanation:**  
# The above cell simulates five text prompts to produce five 512-dimensional embeddings. By applying PCA, we reduce these embeddings to 2D and plot them. This visualization helps to explain how different prompts are mapped to distinct regions in the latent space—a critical step for conditional generation in Stable Diffusion.

# %% [markdown]
# ## 2. Noise Generation Demo
#
# **Purpose:**  
# Diffusion models, including Stable Diffusion, start with a random noise tensor. This noise is progressively denoised to produce a final image.
#
# **Demo Description:**  
# We generate a 64×64 noise map and visualize it alongside its histogram. This demonstrates the Gaussian noise distribution that serves as the starting point for the iterative denoising process.
#

# %% [code]
# Generate a 64x64 latent noise map (Gaussian distributed)
noise_map = np.random.randn(64, 64)
plt.figure(figsize=(5,5))
plt.imshow(noise_map, cmap='viridis')
plt.title("Initial Noise Map in Latent Space")
plt.colorbar()
plt.show()

# Plot the histogram of noise values
plt.figure(figsize=(6,4))
plt.hist(noise_map.flatten(), bins=30, color='coral', edgecolor='black')
plt.title("Histogram of Noise Values")
plt.xlabel("Noise Value")
plt.ylabel("Frequency")
plt.show()

# %% [markdown]
# **Explanation:**  
# This cell generates and displays a noise map that serves as the seed for the denoising process in Stable Diffusion. The histogram confirms that the noise is Gaussian distributed, which is a typical assumption in diffusion models.

# %% [markdown]
# ## 3. Denoising with UNet – Baseline vs. Optimized
#
# **Purpose:**  
# The core of Stable Diffusion lies in iteratively denoising a latent representation using a UNet architecture. Optimizing this process is critical for performance.
#
# **Demo Description:**  
# We simulate two versions of a denoising step:
# - **Baseline Implementation:** Uses two separate convolutional operations.
# - **Optimized Implementation:** Uses a fused convolution to simulate reduced memory operations.
#
# We benchmark these implementations on the GPU and visualize the performance gains.
#

# %% [code]
import torch
import torch.nn as nn
import torch.nn.functional as F
import time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on device:", device)

# Define a baseline denoising module with two convolution layers.
class BaselineDenoise(nn.Module):
    def __init__(self, channels=64):
        super(BaselineDenoise, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

# Define an optimized denoising module that fuses operations.
class OptimizedDenoise(nn.Module):
    def __init__(self, channels=64):
        super(OptimizedDenoise, self).__init__()
        self.fused_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
    def forward(self, x):
        x = self.relu(self.fused_conv(x))
        return x

# Benchmark function to measure average inference time.
def benchmark_model(model, input_tensor, iterations=100):
    # Warm-up
    for _ in range(10):
        _ = model(input_tensor)
    torch.cuda.synchronize()
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    start_event.record()
    for _ in range(iterations):
        _ = model(input_tensor)
    end_event.record()
    torch.cuda.synchronize()
    elapsed_time = start_event.elapsed_time(end_event)  # in ms
    return elapsed_time / iterations

channels = 64
baseline_model = BaselineDenoise(channels).to(device)
optimized_model = OptimizedDenoise(channels).to(device)

# Create a dummy latent input tensor.
input_tensor = torch.randn(1, channels, 64, 64, device=device)

baseline_time = benchmark_model(baseline_model, input_tensor)
optimized_time = benchmark_model(optimized_model, input_tensor)

print(f"Baseline average time per step: {baseline_time:.2f} ms")
print(f"Optimized average time per step: {optimized_time:.2f} ms")

# %% [markdown]
# ### Plotting Denoising Performance
#
# **Explanation:**  
# The bar chart below compares the average inference time per denoising step between the baseline (two-step) and optimized (fused) models. The optimized approach simulates a GPU acceleration technique that is key to making Stable Diffusion more efficient.

# %% [code]
methods = ['Baseline', 'Optimized']
times = [baseline_time, optimized_time]

plt.figure(figsize=(6,4))
bars = plt.bar(methods, times, color=['lightcoral', 'lightseagreen'])
plt.ylabel("Average Time per Step (ms)")
plt.title("Denoising Performance: Baseline vs. Optimized")
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f"{yval:.2f}", ha='center', va='bottom')
plt.show()

# %% [markdown]
# ## 4. Partially Fused Softmax Optimization Demo
#
# **Purpose:**  
# In attention mechanisms (common in transformer-based text embedders and cross-attention in UNets), softmax is a key operation. Optimizing softmax via fusion can reduce memory overhead.
#
# **Demo Description:**  
# We compare the traditional PyTorch softmax with a simulated fused softmax operation, then benchmark their performance.
#

# %% [code]
# Traditional softmax using PyTorch's built-in function.
def traditional_softmax(x):
    return F.softmax(x, dim=-1)

# Create a random tensor to simulate attention scores.
attention_scores = torch.randn(32, 64, device=device)

# Define a simulated partially fused softmax module.
class FusedSoftmax(nn.Module):
    def __init__(self):
        super(FusedSoftmax, self).__init__()
    def forward(self, x):
        max_val, _ = torch.max(x, dim=-1, keepdim=True)
        exps = torch.exp(x - max_val)
        sum_exps = torch.sum(exps, dim=-1, keepdim=True)
        return exps / sum_exps

fused_softmax = FusedSoftmax().to(device)

# Benchmark function for softmax operations.
def benchmark_softmax(func, input_tensor, iterations=1000):
    for _ in range(10):
        _ = func(input_tensor)
    torch.cuda.synchronize()
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    start_event.record()
    for _ in range(iterations):
        _ = func(input_tensor)
    end_event.record()
    torch.cuda.synchronize()
    elapsed_time = start_event.elapsed_time(end_event)
    return elapsed_time / iterations

traditional_time = benchmark_softmax(traditional_softmax, attention_scores)
fused_time = benchmark_softmax(fused_softmax, attention_scores)

print(f"Traditional softmax avg time: {traditional_time:.4f} ms")
print(f"Fused softmax avg time: {fused_time:.4f} ms")

plt.figure(figsize=(6,4))
plt.bar(['Traditional', 'Fused'], [traditional_time, fused_time], color=['salmon', 'seagreen'])
plt.ylabel("Average Time (ms)")
plt.title("Softmax Optimization Performance")
for i, t in enumerate([traditional_time, fused_time]):
    plt.text(i, t + 0.001, f"{t:.4f}", ha='center', va='bottom')
plt.show()

# %% [markdown]
# **Explanation:**  
# The above cell demonstrates and compares the performance of the traditional softmax with a simulated fused version. This optimization is crucial when attention mechanisms are used in Stable Diffusion for efficiently merging text and image features.

# %% [markdown]
# ## 5. Image Decoding Demo
#
# **Purpose:**  
# After iterative denoising, the latent representation is decoded into an RGB image. This final stage is essential for generating a visually coherent output.
#
# **Demo Description:**  
# We simulate a simple decoder network (a small autoencoder-like structure) that converts a denoised latent tensor into a three-channel image.
#

# %% [code]
# Simulate a decoder network.
class SimpleDecoder(nn.Module):
    def __init__(self, channels=64, output_channels=3):
        super(SimpleDecoder, self).__init__()
        self.conv = nn.Conv2d(channels, output_channels, kernel_size=3, padding=1)
        self.sigmoid = nn.Sigmoid()  # Map outputs to [0,1] range.
    def forward(self, x):
        x = self.conv(x)
        x = self.sigmoid(x)
        return x

decoder = SimpleDecoder(channels=64, output_channels=3).to(device)

# Create a dummy denoised latent representation.
denoised_latent = torch.randn(1, channels, 64, 64, device=device)
decoded_image = decoder(denoised_latent).detach().cpu().numpy()[0].transpose(1,2,0)

plt.figure(figsize=(6,6))
plt.imshow(decoded_image)
plt.title("Decoded Image from Denoised Latent")
plt.axis('off')
plt.show()

# %% [markdown]
# **Explanation:**  
# This cell simulates the final decoding step in Stable Diffusion. A denoised latent tensor is passed through a simple decoder to produce an RGB image, mimicking how a high-fidelity image is reconstructed from latent representations.

# %% [markdown]
# ## 6. Extra Optimization: Winograd Convolution Demo
#
# **Purpose:**  
# Winograd convolution reduces the number of multiplications needed in convolution operations—a key optimization for large-scale diffusion models.
#
# **Demo Description:**  
# We simulate the performance difference between a baseline convolution and a "Winograd-optimized" convolution. Although the underlying PyTorch operations remain the same here, the timing simulation represents the benefits of reduced multiplications (at the cost of higher memory usage).
#

# %% [code]
# Baseline convolution operation.
class BaselineConv(nn.Module):
    def __init__(self, in_channels=64, out_channels=64):
        super(BaselineConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
    def forward(self, x):
        return self.conv(x)

# Simulated Winograd convolution.
class WinogradConv(nn.Module):
    def __init__(self, in_channels=64, out_channels=64):
        super(WinogradConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
    def forward(self, x):
        # In a real implementation, transformation matrices reduce multiplications.
        return self.conv(x)

baseline_conv = BaselineConv().to(device)
winograd_conv = WinogradConv().to(device)

# Create dummy input for convolution benchmarking.
input_conv = torch.randn(1, 64, 64, 64, device=device)
baseline_conv_time = benchmark_model(baseline_conv, input_conv, iterations=100)
winograd_conv_time = benchmark_model(winograd_conv, input_conv, iterations=100)

print(f"Baseline convolution time: {baseline_conv_time:.2f} ms")
print(f"Simulated Winograd convolution time: {winograd_conv_time:.2f} ms")

plt.figure(figsize=(6,4))
plt.bar(['Baseline Conv', 'Winograd Conv'], [baseline_conv_time, winograd_conv_time], color=['orchid', 'teal'])
plt.ylabel("Average Time per Step (ms)")
plt.title("Convolution Optimization Performance")
for i, t in enumerate([baseline_conv_time, winograd_conv_time]):
    plt.text(i, t + 1, f"{t:.2f}", ha='center', va='bottom')
plt.show()

# %% [markdown]
# **Explanation:**  
# Here we simulate a Winograd convolution, which in production would reduce the number of multiplications needed during convolution operations. The performance metrics in this demo represent the potential speedups achievable with such optimizations in diffusion models.

# %% [markdown]
# ## 7. Iterative Denoising Animation
#
# **Purpose:**  
# The iterative denoising process is the core of diffusion models like Stable Diffusion. This stage gradually refines a noisy image to produce a coherent final output.
#
# **Demo Description:**  
# We animate the iterative denoising process using a Gaussian blur as a stand-in for a denoising step. This visualizes how repeated refinements transform an initial noisy latent into a clearer image.
#

# %% [code]
import cv2
from matplotlib import animation

# Create an initial noisy image to simulate a latent image.
image_size = 128
noisy_image = np.random.rand(image_size, image_size)

def denoise_step(img, ksize=5):
    # Use OpenCV Gaussian blur to simulate a denoising operation.
    denoised = cv2.GaussianBlur(img, (ksize, ksize), 0)
    return denoised

fig, ax = plt.subplots(figsize=(5,5))
im = ax.imshow(noisy_image, cmap='gray', animated=True)
ax.set_title("Iterative Denoising Simulation")
ax.axis('off')

def update(frame):
    global noisy_image
    noisy_image = denoise_step(noisy_image, ksize=5)
    im.set_array(noisy_image)
    ax.set_title(f"Iteration {frame}")
    return [im]

ani = animation.FuncAnimation(fig, update, frames=30, interval=200, blit=True)
plt.show()

# %% [markdown]
# ## Summary
#
# In this notebook, we demonstrated a series of GPU optimization techniques that are integral to improving the performance of diffusion models such as Stable Diffusion:
# 
# - **Text Embedding:** We simulated converting text prompts into high-dimensional embeddings and visualized their latent space via PCA.
# - **Noise Generation:** We examined the Gaussian noise distribution used as the starting point for diffusion.
# - **Denoising Operations:** We compared a baseline UNet denoising step with an optimized fused version and demonstrated a simulated partially fused softmax for attention.
# - **Image Decoding:** We showed how a denoised latent representation is converted into an RGB image.
# - **Extra Optimizations:** We simulated Winograd convolution to illustrate further computational efficiencies.
# - **Iterative Denoising Animation:** We animated the iterative denoising process, the heart of the diffusion model.
#
# These demos provide both visual and performance metrics that highlight the importance of GPU optimizations in enabling efficient and scalable diffusion models.
#
# **Enjoy exploring these demos and feel free to modify or extend them for your presentations!**